{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(list=ls())\n",
    "setwd(\"<path to data>\")\n",
    "list.files()\n",
    "\n",
    "####################\n",
    "####################\n",
    "## Load libraries ##\n",
    "####################\n",
    "####################\n",
    "\n",
    "library(ggplot2)\n",
    "library(gamm4)\n",
    "library(MASS)\n",
    "library(car)\n",
    "library(plyr)\n",
    "library(TOSTER)\n",
    "library(forcats)\n",
    "library(QuantPsyc)\n",
    "library(data.table)\n",
    "library(reshape2)\n",
    "library(psych)\n",
    "\n",
    "options(max.print=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "###################################\n",
    "## Load and manipulate ABCD data ##\n",
    "###################################\n",
    "###################################\n",
    "###\n",
    "\n",
    "###################################\n",
    "##Data sets and variables used\n",
    "\n",
    "#This analysis begins after merging of data, recovery of categorical variables,\n",
    "#imputation of missing core demographic variables, and recoding of core demographic variables. The\n",
    "#scripts to conduct this preliminary analysis are available at https://github.com/ABCD-STUDY/analysis-nda17.\n",
    "#Data from the ABCD Youth Acculturation Survey were unmodified, except as described below.\n",
    "\n",
    "#Variables used in this study:\n",
    "#\n",
    "#completedData$src_subject_id\n",
    "#completedData$age\n",
    "#completedData$female\n",
    "#completedData$household.income\n",
    "#completedData$high.educ\n",
    "#completedData$married\n",
    "#completedData$race.ethnicity\n",
    "#nda17$demo_origin_v2\n",
    "#nda17$demo_prnt_16\n",
    "#nda17$demo_biofather_v2\n",
    "#nda17$demo_biomother_v2\n",
    "#nda17$nihtbx_reading_uncorrected\n",
    "#nda17$nihtbx_picture_uncorrected\n",
    "#nda17$nihtbx_list_uncorrected\n",
    "#nda17$nihtbx_pattern_uncorrected\n",
    "#nda17$nihtbx_picvocab_uncorrected\n",
    "#nda17$nihtbx_flanker_uncorrected\n",
    "#nda17$nihtbx_cardsort_uncorrected\n",
    "#nda17$beh_sst_ssrt_mean_total\n",
    "#nda17$abcd_site\n",
    "#nda17$rel_family_id\n",
    "#nda17$accult_q1_y\n",
    "#nda17$accult_q2_y\n",
    "#nda17$accult_q3_dropdwn_y\n",
    "#nda17$accult_q3_other_y\n",
    "#nda17$accult_q4_y\n",
    "#nda17$accult_q5_y\n",
    "\n",
    "#Data about bilingualism are taken from the Youth Acculturation Survey. The original coding is below:\n",
    "#accult_q1_y: \"How well do you speak English?\" 1, Poor | 2, Fair | 3, Good | 4, Excellent\n",
    "#accult_q2_y: \"Besides English, do you speak or understand another language or dialect?\"\n",
    "#accult_q3_other_y and accult_q3_dropdown_y: \"What other language or dialect do you speak or understand (besides English)?\"\n",
    "#accult_q4_y: \"What language do you speak with most of your friends?\" 1, (<em>Other language</em>) all the time | 2, (<em>Other language</em>) most of the time | 3, (<em>Other language</em>) and English equally | 4, English most of the time | 5, English all the time\n",
    "#accult_q5_y: \"What language do you speak with most of your family?\" 1, (<em>Other language</em>) all the time | 2, (<em>Other language</em>) most of the time | 5, (<em>Other language</em>) and English equally | 4, English most of the time | 5, English all the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nda17 =  readRDS(\"nda17.Rds\")\n",
    "attach(nda17)\n",
    "completedData =  readRDS(\"completedData.Rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recode some variables to reflect the original data scale, to deal with NAs, and to recode answers that are clear coding mistakes\n",
    "\n",
    "accult_q1_y <- factor(ifelse(nda17$accult_q1_y == \"\", NA, ifelse(nda17$accult_q1_y == \"Poor\", 0, ifelse(nda17$accult_q1_y == \"Fair\", 1,ifelse(nda17$accult_q1_y == \"Good\", 2, ifelse(nda17$accult_q1_y == \"Excellent\", 3, NA))))))\n",
    "accult_q2_y <- factor(ifelse(nda17$accult_q2_y == \"\", NA, ifelse(nda17$accult_q2_y == \"no\", 0, ifelse(nda17$accult_q2_y == \"yes\", 1,ifelse(nda17$accult_q2_y == \"decline to answer\", NA, ifelse(nda17$accult_q2_y == \"refuse to answer\", NA, NA))))))\n",
    "accult_q4_y <- factor(ifelse(nda17$accult_q4_y == \"\", NA, ifelse(nda17$accult_q4_y == \"(Other language) all the time\", 1, ifelse(nda17$accult_q4_y == \"(Other language) most of the time\", 2,ifelse(nda17$accult_q4_y == \"(Other language) and English equally\", 3, ifelse(nda17$accult_q4_y == \"English most of the time\", 4, ifelse(nda17$accult_q4_y == \"English all the time\", 5, NA)))))))\n",
    "accult_q5_y <- factor(ifelse(nda17$accult_q5_y == \"\", NA, ifelse(nda17$accult_q5_y == \"(Other language) all the time\", 1, ifelse(nda17$accult_q5_y == \"(Other language) most of the time\", 2,ifelse(nda17$accult_q5_y == \"(Other language) and English equally\", 3, ifelse(nda17$accult_q5_y == \"English most of the time\", 4, ifelse(nda17$accult_q5_y == \"English all the time\", 5, NA)))))))\n",
    "accult_q2_y[which(accult_q3_other_y==\"Pig Latin\")]<-\"0\"\n",
    "accult_q2_y[which(accult_q3_other_y==\"A little bit\")]<-\"0\"\n",
    "accult_q2_y[which(accult_q3_other_y==\"British\")]<-\"0\"\n",
    "accult_q2_y[which(accult_q3_other_y==\"English\")]<-\"0\"\n",
    "SSRTr<-(beh_sst_ssrt_mean_total) * -1 #Stop-Signal reaction time, recode: recode/reverse score SSRT (make lower SSRT mean poorer performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a smaller data frame that is easier to manage\n",
    "\n",
    "abcd_subset<-data.frame(completedData$src_subject_id, as.numeric(nda17$nihtbx_reading_uncorrected), as.numeric(nda17$nihtbx_picture_uncorrected), as.numeric(nda17$nihtbx_list_uncorrected),\n",
    "as.numeric(nda17$nihtbx_pattern_uncorrected), as.numeric(nda17$nihtbx_cryst_uncorrected), as.numeric(nda17$nihtbx_fluidcomp_uncorrected), completedData$age, completedData$female,\n",
    "completedData$race.ethnicity, completedData$high.educ, completedData$married, completedData$household.income, as.numeric(nda17$nihtbx_picvocab_uncorrected), as.numeric(nda17$nihtbx_flanker_uncorrected),\n",
    "as.numeric(nda17$nihtbx_cardsort_uncorrected), as.numeric(nda17$beh_sst_ssrt_mean_total), SSRTr, nda17$abcd_site, nda17$rel_family_id, accult_q1_y, accult_q2_y, nda17$accult_q3_dropdwn_y, nda17$accult_q3_other_y, accult_q4_y, accult_q5_y)\n",
    "\n",
    "colnames(abcd_subset)<-c(\"src_subject_id\", \"nihtbx_reading_uncorrected\", \"nihtbx_picture_uncorrected\", \"nihtbx_list_uncorrected\", \"nihtbx_pattern_uncorrected\",\n",
    "\"nihtbx_cryst_uncorrected\", \"nihtbx_fluidcomp_uncorrected\", \"age\", \"female\", \"race.ethnicity\", \"high.educ\", \"married\", \"household.income\",\n",
    "\"nihtbx_picvocab_uncorrected\", \"nihtbx_flanker_uncorrected\", \"nihtbx_cardsort_uncorrected\", \"beh_sst_ssrt_mean_total\", \"SSRTr\", \"abcd_site\",\"rel_family_id\",\"accult_q1_y\",\"accult_q2_y\",\n",
    "\"accult_q3_dropdwn_y\",\"accult_q3_other_y\",\"accult_q4_y\",\"accult_q5_y\")\n",
    "\n",
    "#saveRDS(abcd_subset, \"<filename>.Rds\") # save this dataframe for future use\n",
    "#abcd_subset<-readRDS(\"<filename>.Rds\") # load the dataframe if you come back to the analysis at a later point so you can start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names(abcd_subset)\n",
    "dim(abcd_subset)\n",
    "attach(abcd_subset)\n",
    "Hmisc::describe(abcd_subset)\n",
    "\n",
    "count(accult_q1_y)\n",
    "count(accult_q2_y)\n",
    "count(accult_q3_other_y)\n",
    "count(accult_q3_dropdwn_y)\n",
    "count(accult_q4_y)\n",
    "count(accult_q5_y)\n",
    "count(nda17$demo_origin_v2)\n",
    "count(nda17$demo_prnt_16)\n",
    "count(nda17$demo_biofather_v2) #\n",
    "count(nda17$demo_biomother_v2) #\n",
    "\n",
    "#recode the accult_q2_y variable into a binary \"Bilingual Status\", 0 = not bilingual; 1 = bilingual\n",
    "\n",
    "bilingual_status <- accult_q2_y\n",
    "\n",
    "#dimension a 'bilingual degree' variable, where 1 = participant said they were bilingual, and they speak the other language with friends all the time, most of the time,\n",
    "#or equally, OR they speak the other language with family all the time, most of the time, or equally.\n",
    "\n",
    "bilingual_degree <- ifelse(bilingual_status == 0, 0, ifelse(bilingual_status == 1 & (as.numeric(accult_q4_y) <= 3 | as.numeric(accult_q5_y) <= 3), 1, NA))\n",
    "\n",
    "count(bilingual_degree) #check the data\n",
    "\n",
    "#dimension a continuous 'bilingual use' variable, and reverse-score so that if participants speak the other language with friends all the time, most of the time...,\n",
    "#they will receive high scores on this measure (range 0-8, with 8 indicating a high-degree of other language use)\n",
    "\n",
    "bilingual_use<-10-(as.numeric(abcd_subset$accult_q4_y)+as.numeric(abcd_subset$accult_q5_y))\n",
    "\n",
    "count(bilingual_use) # check the data\n",
    "for.hist.use <- melt(bilingual_use)\n",
    "tiff(\"figure_use.tiff\", units = 'in', width = 12, height = 10, res = 300, compression = \"lzw\")\n",
    "par(mar = c(5, 5, 8, 8), xpd=FALSE) #set figure boundaries\n",
    "ggplot(for.hist.use,aes(x = for.hist.use$value)) + theme_bw() + geom_histogram(\n",
    "    col = \"grey\", bins = 17,\n",
    "    fill=\"light blue\") +\n",
    "    labs(x=\"Score\", y=\"Frequency\") +\n",
    "    ggtitle(\"Frequency of Scores for Bilingual Use Variable\") +\n",
    "  \ttheme(plot.title = element_text(hjust = 0.5), text = element_text(size=20), axis.text = element_text(size = 14, colour = \"black\"))\n",
    "dev.off()\n",
    "\n",
    "#compute new IQ measure\n",
    "\n",
    "fluidIQ<-scale((nihtbx_picture_uncorrected+nihtbx_list_uncorrected+nihtbx_pattern_uncorrected)/3)\n",
    "hist(fluidIQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "###################################################\n",
    "##Select Variables for OLS and Multilevel Models###\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "#abcd_subset<-cbind.data.frame(abcd_subset, fluidIQ, bilingual_status, bilingual_degree, bilingual_use) # create a new data frame\n",
    "abcd_subset<-cbind.data.frame(abcd_subset, fluidIQ, bilingual_status, bilingual_degree, bilingual_use) # create a new data frame\n",
    "\n",
    "###### Select covariates for model  ######\n",
    "\n",
    "ind_cov = c(which(names(abcd_subset)==\"age\"),which(names(abcd_subset)==\"female\"),which(names(abcd_subset)==\"race.ethnicity\"), which(names(abcd_subset)==\"high.educ\"),which(names(abcd_subset)==\"married\"),which(names(abcd_subset)==\"household.income\"))\n",
    "names(abcd_subset)[ind_cov]\n",
    "summary(abcd_subset[,ind_cov])\n",
    "\n",
    "###### Select covariates for model, including IQ  ######\n",
    "\n",
    "#ind_cov = c(which(names(abcd_subset)==\"nihtbx_reading_uncorrected\"), which(names(abcd_subset)==\"fluidIQ\"), which(names(abcd_subset)==\"age\"),which(names(abcd_subset)==\"female\"),which(names(abcd_subset)==\"race.thnicity\"), which(names(abcd_subset)==\"high.educ\"),which(names(abcd_subset)==\"married\"),which(names(abcd_subset)==\"household.income\"))\n",
    "\n",
    "## Select covariates for model, including Picture Vocabulary and IQ\n",
    "\n",
    "#ind_cov = c(which(names(abcd_subset)==\"nihtbx_reading_uncorrected\"), which(names(abcd_subset)==\"fluidIQ\"), which(names(abcd_subset)==\"nihtbx_picvocab_uncorrected\"), which(names(abcd_subset)==\"age\"),which(names(abcd_subset)==\"female\"),which(names(abcd_subset)==\"race.thnicity\"), which(names(abcd_subset)==\"high.educ\"),which(names(abcd_subset)==\"married\"),which(names(abcd_subset)==\"household.income\"))\n",
    "\n",
    "## Select nesting variables\n",
    "\n",
    "ind_nest = c(which(names(abcd_subset)==\"abcd_site\"),which(names(abcd_subset)==\"rel_family_id\"));names(abcd_subset)[ind_nest]\n",
    "\n",
    "###### Select DVs  ######\n",
    "\n",
    "ind_dv = c(which(names(abcd_subset)==\"nihtbx_picvocab_uncorrected\"),which(names(abcd_subset)==\"nihtbx_flanker_uncorrected\"),which(names(abcd_subset)==\"nihtbx_cardsort_uncorrected\"), which(names(abcd_subset)==\"SSRTr\"))\n",
    "\n",
    "#ind_dv = c(which(names(abcd_subset)==\"nihtbx_flanker_uncorrected\"),which(names(abcd_subset)==\"nihtbx_cardsort_uncorrected\"), which(names(abcd_subset)==\"SSRTr\"))\n",
    "\n",
    "#for(j in 1:length(ind_dv)) abcd_subset[,ind_dv[j]] = scale(as.numeric(abcd_subset[,ind_dv[j]])) # standardize the DV to get standardized betas\n",
    "\n",
    "names(abcd_subset)[ind_dv]\n",
    "boxplot(abcd_subset[ind_dv])\n",
    "summary(abcd_subset[ind_dv])\n",
    "\n",
    "###### Select IVs  ######\n",
    "\n",
    "ind_iv = c(which(names(abcd_subset)==\"bilingual_status\"),which(names(abcd_subset)==\"bilingual_degree\"), which(names(abcd_subset)==\"bilingual_use\"))\n",
    "\n",
    "#for(k in 1:length(ind_iv)) abcd_subset[,ind_iv[k]] = scale(as.numeric(abcd_subset[,ind_iv[k]])) # standardize the IV to get standardized betas\n",
    "\n",
    "names(abcd_subset)[ind_iv]\n",
    "summary(abcd_subset[,ind_iv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "####################################\n",
    "## Run OLS and Multilevel Models  ##\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "#run once with original units, once with standardized units\n",
    "\n",
    "pause = function()\n",
    "{\n",
    "    if (interactive())\n",
    "    {\n",
    "        invisible(readline(prompt = \"Press <Enter> to continue...\"))\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        cat(\"Press <Enter> to continue...\")\n",
    "        invisible(readLines(file(\"stdin\"), 1))\n",
    "    }\n",
    "}\n",
    "m <- matrix(nrow=33, ncol=9) # set the output matrix size to hold the results\n",
    "mat_row = 25 # to fill out the m matrix, start at row 13, row 25 when running with different covariates\n",
    "for(i in 1:length(ind_dv)){\n",
    "print(i)\n",
    "\tfor(j in 1:length(ind_iv)){\n",
    "\tprint(j)\n",
    "\t\tpred<-paste(names(abcd_subset)[ind_iv[j]],\"-->\",names(abcd_subset)[ind_dv[i]])\n",
    "\t\tprint(pred)\n",
    "\t\tform = paste(names(abcd_subset)[ind_dv[i]],\"~\" ,names(abcd_subset)[ind_iv[j]])\n",
    "\t\tform.simple = formula(form)\n",
    "\t\t\tfor(k in 1:length(ind_cov)){\n",
    "\t\t\t\tform = paste(form,\"+\",names(abcd_subset)[ind_cov[k]])\n",
    "\t\t\t\t\t}\n",
    "\t\tprint(form)\n",
    "\t\tform = formula(form)\n",
    "\t\tran = paste(\"~(1|\",names(abcd_subset)[ind_nest[1]])\n",
    "\t\t\tfor(k in 2:length(ind_nest)){\n",
    "\t\t\t\tran = paste(ran,\"/\",names(abcd_subset)[ind_nest[k]])\n",
    "\t\t\t\t\t}\n",
    "\t\tran = paste(ran,\")\")\n",
    "\t\tran = formula(ran)\n",
    "\t\t\tmod0 = gamm4(formula = form.simple, random = ran, data = abcd_subset)\n",
    "\t\t\tprint(summary(mod0$gam))\n",
    "\t\t\t#pause()\n",
    "\t\t\tgamm0 = gamm4(formula = form, random = ran, data = abcd_subset)\n",
    "\t\t\t#pause()\n",
    "\t\t\tprint(summary(gamm0$gam))\n",
    "\t\t\toutput<-c(pred,summary(mod0$gam)$p.coeff[2],summary(mod0$gam)$se[2],summary(mod0$gam)$p.t[2],summary(mod0$gam)$p.pv[2],summary(gamm0$gam)$p.coeff[2],summary(gamm0$gam)$se[2],summary(gamm0$gam)$p.t[2],summary(gamm0$gam)$p.pv[2])\n",
    "\t\t\tm[mat_row,]<-output # place the output into the matrix that was dimensioned above\n",
    "\t\t\tmat_row = mat_row + 1 # step to the next matrix row\n",
    "\t\t#print(\"########################################\")\n",
    "\t\t#print(\"########################################\")\n",
    "\t}\n",
    "}\n",
    "colnames(m)<-c(\"prediction\",\"nocov_b\",\"nocov_se\",\"nocov_tval\",\"nocov_pval\",\"gamm_b\",\"gamm_se\",\"gamm_tval\",\"gamm_pval\")\n",
    "print(m)\n",
    "write.table(m,file=\"filename.txt\") # save for filling out tables, figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "################################################\n",
    "## Tests of Equivalence                       ##\n",
    "################################################\n",
    "################################################\n",
    "\n",
    "## The function below uses regression slopes not raw data\n",
    "## Based on Counsell, A., & Cribbie, R. A. (2014). Equivalence tests for comparing correlation and regression coefficients.\n",
    "## British Journal of Mathematical and Statistical Psychology, 68, 292-309.\n",
    "## Special thanks to Alyssa Counsell for sharing the code\n",
    "\n",
    "# b1 is the regression coefficient for group 1\n",
    "# b2 is the regression coefficient for group 2\n",
    "# se1 is the standard error of the regression coefficient for group 1\n",
    "# se2 is the standard error of the regression coefficient for group 2\n",
    "# equiv_int is the equivalence interval\n",
    "\n",
    "equivbs<-function(b1,b2,se1,se2,equiv_int,alpha=.05){\n",
    "\tp.value<-pnorm((abs(b1-b2)-equiv_int)/(sqrt(se1^2+se2^2)))-pnorm((-abs(b1-b2)-equiv_int)/(sqrt(se1^2+se2^2)))\n",
    "\tupper<-(b1-b2)+qnorm(alpha)*(sqrt(se1^2+se2^2))\n",
    "\tlower<-(b1-b2)-qnorm(alpha)*(sqrt(se1^2+se2^2))\n",
    "\t\tif (lower<upper) {\n",
    "\t\t\tlower2<-lower\n",
    "\t\t\tupper2<-upper\n",
    "\t\t\t}\n",
    "\t\tif(lower>upper){\n",
    "\t\t\tlower2<-upper\n",
    "\t\t\tupper2<-lower\n",
    "\t\t\t}\n",
    "\tCI<-c(lower2,upper2)\n",
    "\tifelse (p.value<=alpha, decision<-\"The null hypothesis that the difference between the groups' regression coefficients falls outside of the equivalence interval can be rejected.The groups' regression coefficients can be considered equivalent\", decision<-\"The null hypothesis that the difference between the groups' regression coefficients falls outside of the equivalence interval cannot be rejected.The groups' regression coefficients can NOT be considered equivalent.\")\n",
    "\tout<-list( p.value, CI,decision)\n",
    "\tnames(out)<-c(\"P value\", \"CI\", \"Decision\")\n",
    "print(out)\n",
    "}\n",
    "########\n",
    "#use the m matrix from the regressions to fill out the figure\n",
    "\n",
    "y<-rev(c(NA, NA, as.numeric(m[1,2]),\n",
    "as.numeric(m[2,2]),\n",
    "as.numeric(m[3,2]),\n",
    "NA,\n",
    "as.numeric(m[4,2]),\n",
    "as.numeric(m[5,2]),\n",
    "as.numeric(m[6,2]),\n",
    "NA,\n",
    "as.numeric(m[7,2]),\n",
    "as.numeric(m[8,2]),\n",
    "as.numeric(m[9,2]),\n",
    "NA,\n",
    "as.numeric(m[10,2]),\n",
    "as.numeric(m[11,2]),\n",
    "as.numeric(m[12,2]),\n",
    "NA, NA, NA,\n",
    "as.numeric(m[1,6]),\n",
    "as.numeric(m[2,6]),\n",
    "as.numeric(m[3,6]),\n",
    "NA,\n",
    "as.numeric(m[4,6]),\n",
    "as.numeric(m[5,6]),\n",
    "as.numeric(m[6,6]),\n",
    "NA,\n",
    "as.numeric(m[7,6]),\n",
    "as.numeric(m[8,6]),\n",
    "as.numeric(m[9,6]),\n",
    "NA,\n",
    "as.numeric(m[10,6]),\n",
    "as.numeric(m[11,6]),\n",
    "as.numeric(m[12,6]),\n",
    "NA, NA, NA,\n",
    "as.numeric(m[13,6]),\n",
    "as.numeric(m[14,6]),\n",
    "as.numeric(m[15,6]),\n",
    "NA,\n",
    "as.numeric(m[16,6]),\n",
    "as.numeric(m[17,6]),\n",
    "as.numeric(m[18,6]),\n",
    "NA,\n",
    "as.numeric(m[19,6]),\n",
    "as.numeric(m[20,6]),\n",
    "as.numeric(m[21,6]),\n",
    "NA,\n",
    "as.numeric(m[22,6]),\n",
    "as.numeric(m[23,6]),\n",
    "as.numeric(m[24,6]),\n",
    "NA, NA, NA,\n",
    "as.numeric(m[25,6]),\n",
    "as.numeric(m[26,6]),\n",
    "as.numeric(m[27,6]),\n",
    "NA,\n",
    "as.numeric(m[28,6]),\n",
    "as.numeric(m[29,6]),\n",
    "as.numeric(m[30,6]),\n",
    "NA,\n",
    "as.numeric(m[31,6]),\n",
    "as.numeric(m[32,6]),\n",
    "as.numeric(m[33,6])))\n",
    "\n",
    "ylo<-rev(c(NA, NA, equivbs(as.numeric(m[1,2]),0,as.numeric(m[1,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[2,2]),0,as.numeric(m[2,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[3,2]),0,as.numeric(m[3,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[4,2]),0,as.numeric(m[4,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[5,2]),0,as.numeric(m[5,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[6,2]),0,as.numeric(m[6,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[7,2]),0,as.numeric(m[7,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[8,2]),0,as.numeric(m[8,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[9,2]),0,as.numeric(m[9,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[10,2]),0,as.numeric(m[10,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[11,2]),0,as.numeric(m[11,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[12,2]),0,as.numeric(m[12,3]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[1,6]),0,as.numeric(m[1,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[2,6]),0,as.numeric(m[2,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[3,6]),0,as.numeric(m[3,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[4,6]),0,as.numeric(m[4,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[5,6]),0,as.numeric(m[5,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[6,6]),0,as.numeric(m[6,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[7,6]),0,as.numeric(m[7,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[8,6]),0,as.numeric(m[8,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[9,6]),0,as.numeric(m[9,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[10,6]),0,as.numeric(m[10,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[11,6]),0,as.numeric(m[11,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[12,6]),0,as.numeric(m[12,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[13,6]),0,as.numeric(m[13,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[14,6]),0,as.numeric(m[14,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[15,6]),0,as.numeric(m[15,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[16,6]),0,as.numeric(m[16,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[17,6]),0,as.numeric(m[17,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[18,6]),0,as.numeric(m[18,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[19,6]),0,as.numeric(m[19,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[20,6]),0,as.numeric(m[20,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[21,6]),0,as.numeric(m[21,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[22,6]),0,as.numeric(m[22,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[23,6]),0,as.numeric(m[23,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[24,6]),0,as.numeric(m[24,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[25,6]),0,as.numeric(m[25,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[26,6]),0,as.numeric(m[26,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[27,6]),0,as.numeric(m[27,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[28,6]),0,as.numeric(m[28,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[29,6]),0,as.numeric(m[29,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[30,6]),0,as.numeric(m[30,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "NA,\n",
    "equivbs(as.numeric(m[31,6]),0,as.numeric(m[31,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[32,6]),0,as.numeric(m[32,7]),0,c(-.1, .1),alpha = .05)$`CI`[1],\n",
    "equivbs(as.numeric(m[33,6]),0,as.numeric(m[33,7]),0,c(-.1, .1),alpha = .05)$`CI`[1]))\n",
    "\n",
    "yhi<-rev(c(NA, NA, equivbs(as.numeric(m[1,2]),0,as.numeric(m[1,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[2,2]),0,as.numeric(m[2,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[3,2]),0,as.numeric(m[3,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[4,2]),0,as.numeric(m[4,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[5,2]),0,as.numeric(m[5,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[6,2]),0,as.numeric(m[6,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[7,2]),0,as.numeric(m[7,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[8,2]),0,as.numeric(m[8,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[9,2]),0,as.numeric(m[9,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[10,2]),0,as.numeric(m[10,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[11,2]),0,as.numeric(m[11,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[12,2]),0,as.numeric(m[12,3]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[1,6]),0,as.numeric(m[1,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[2,6]),0,as.numeric(m[2,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[3,6]),0,as.numeric(m[3,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[4,6]),0,as.numeric(m[4,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[5,6]),0,as.numeric(m[5,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[6,6]),0,as.numeric(m[6,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[7,6]),0,as.numeric(m[7,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[8,6]),0,as.numeric(m[8,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[9,6]),0,as.numeric(m[9,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[10,6]),0,as.numeric(m[10,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[11,6]),0,as.numeric(m[11,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[12,6]),0,as.numeric(m[12,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[13,6]),0,as.numeric(m[13,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[14,6]),0,as.numeric(m[14,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[15,6]),0,as.numeric(m[15,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[16,6]),0,as.numeric(m[16,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[17,6]),0,as.numeric(m[17,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[18,6]),0,as.numeric(m[18,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[19,6]),0,as.numeric(m[19,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[20,6]),0,as.numeric(m[20,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[21,6]),0,as.numeric(m[21,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[22,6]),0,as.numeric(m[22,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[23,6]),0,as.numeric(m[23,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[24,6]),0,as.numeric(m[24,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA, NA, NA,\n",
    "equivbs(as.numeric(m[25,6]),0,as.numeric(m[25,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[26,6]),0,as.numeric(m[26,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[27,6]),0,as.numeric(m[27,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[28,6]),0,as.numeric(m[28,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[29,6]),0,as.numeric(m[29,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[30,6]),0,as.numeric(m[30,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "NA,\n",
    "equivbs(as.numeric(m[31,6]),0,as.numeric(m[31,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[32,6]),0,as.numeric(m[32,7]),0,c(-.1, .1),alpha = .05)$`CI`[2],\n",
    "equivbs(as.numeric(m[33,6]),0,as.numeric(m[33,7]),0,c(-.1, .1),alpha = .05)$`CI`[2]))\n",
    "\n",
    "x<-as_factor(c(\"Bilingual Use -> SSRTa\", \"Bilingual Degree -> SSRTa\", \"Bilingual Status -> SSRTa\", \"\",\n",
    "\"Bilingual Use -> Card Sorta\", \"Bilingual Degree -> Card Sorta\", \"Bilingual Status -> Card Sorta\", \" \", \"Bilingual Use -> Flankera\", \"Bilingual Degree -> Flankera\",\n",
    "\"Bilingual Status -> Flankera\", \"   \", \"GAMM Model (w covariates 1-9)\", \"    \",\n",
    "\"Bilingual Use -> SSRTb\", \"Bilingual Degree -> SSRTb\", \"Bilingual Status -> SSRTb\", \"     \",\n",
    "\"Bilingual Use -> Card Sortb\", \"Bilingual Degree -> Card Sortb\", \"Bilingual Status -> Card Sortb\", \"      \",\n",
    "\"Bilingual Use -> Flankerb\", \"Bilingual Degree -> Flankerb\", \"Bilingual Status -> Flankerb\", \"       \", \"Bilingual Use -> Vocabularyb\",\n",
    "\"Bilingual Degree -> Vocabularyb\", \"Bilingual Status -> Vocabularyb\", \"        \",\"GAMM Model (w covariates 1-8)\",\n",
    "\"         \", \"Bilingual Use -> SSRTc\", \"Bilingual Degree -> SSRTc\", \"Bilingual Status -> SSRTc\", \"          \", \"Bilingual Use -> Card Sortc\", \"Bilingual Degree -> Card Sortc\",\n",
    "\"Bilingual Status -> Card Sortc\", \"           \",\n",
    "\"Bilingual Use -> Flankerc\", \"Bilingual Degree -> Flankerc\", \"Bilingual Status -> Flankerc\", \"            \", \"Bilingual Use -> Vocabularyc\", \"Bilingual Degree -> Vocabularyc\",\n",
    "\"Bilingual Status -> Vocabularyc\",\"             \", \"GAMM Model (w covariates 1-6)\", \"              \", \"Bilingual Use -> SSRTd\", \"Bilingual Degree -> SSRTd\",\n",
    "\"Bilingual Status -> SSRTd\", \"               \", \"Bilingual Use -> Card Sortd\", \"Bilingual Degree -> Card Sortd\",\n",
    "\"Bilingual Status -> Card Sortd\", \"                \", \"Bilingual Use -> Flankerd\", \"Bilingual Degree -> Flankerd\", \"Bilingual Status -> Flankerd\",\n",
    "\"                  \", \"Bilingual Use -> Vocabularyd\", \"Bilingual Degree -> Vocabularyd\", \"Bilingual Status -> Vocabularyd\", \"                   \", \"OLS Model\"))\n",
    "\n",
    "d<-data.frame(x, y, ylo, yhi)\n",
    "\n",
    "credplot.gg <- function(d){\n",
    " # d is a data frame with 4 columns\n",
    " # d$x gives variable names\n",
    " # d$y gives center point\n",
    " # d$ylo gives lower limits\n",
    " # d$yhi gives upper limits\n",
    " require(ggplot2)\n",
    " theme_set(theme_light(base_size = 22))\n",
    " p <- ggplot(d, aes(x=x, y=y, ymin=ylo, ymax=yhi))+\n",
    " geom_pointrange()+\n",
    " geom_point(shape=21, fill = \"light blue\", col = \"grey\", size=5, stroke = 1) + geom_hline(yintercept = 0, linetype=1) +\n",
    " geom_hline(yintercept = -.1, size = 1, linetype=2) +\n",
    " geom_hline(yintercept = .1, size = 1, linetype=2) +\n",
    " coord_flip()+scale_y_continuous(breaks=seq(-4,4,.1)) +\n",
    " ylab('Effect Size') +\n",
    " scale_x_discrete('Predictor -> Outcome', waiver(), labels = c(\"Bilingual Use -> SSRT\", \"Bilingual Degree -> SSRT\", \"Bilingual Status -> SSRT\", \"\",\n",
    "\"Bilingual Use -> Card Sort\", \"Bilingual Degree -> Card Sort\", \"Bilingual Status -> Card Sort\", \" \", \"Bilingual Use -> Flanker\", \"Bilingual Degree -> Flanker\",\n",
    "\"Bilingual Status -> Flanker\", \"   \", \"GAMM Model (w covariates 1-9)\", \"    \",\n",
    "\"Bilingual Use -> SSRT\", \"Bilingual Degree -> SSRT\", \"Bilingual Status -> SSRT\", \"     \",\n",
    "\"Bilingual Use -> Card Sort\", \"Bilingual Degree -> Card Sort\", \"Bilingual Status -> Card Sort\", \"      \",\n",
    "\"Bilingual Use -> Flanker\", \"Bilingual Degree -> Flanker\", \"Bilingual Status -> Flanker\", \"       \", \"Bilingual Use -> Vocabulary\",\n",
    "\"Bilingual Degree -> Vocabulary\", \"Bilingual Status -> Vocabulary\", \"        \",\"GAMM Model (w covariates 1-8)\",\n",
    "\"         \", \"Bilingual Use -> SSRT\", \"Bilingual Degree -> SSRT\", \"Bilingual Status -> SSRT\", \"          \", \"Bilingual Use -> Card Sort\", \"Bilingual Degree -> Card Sort\",\n",
    "\"Bilingual Status -> Card Sort\", \"           \",\n",
    "\"Bilingual Use -> Flanker\", \"Bilingual Degree -> Flanker\", \"Bilingual Status -> Flanker\", \"            \", \"Bilingual Use -> Vocabulary\", \"Bilingual Degree -> Vocabulary\",\n",
    "\"Bilingual Status -> Vocabulary\",\"             \", \"GAMM Model (w covariates 1-6)\", \"              \", \"Bilingual Use -> SSRT\", \"Bilingual Degree -> SSRT\",\n",
    "\"Bilingual Status -> SSRT\", \"               \", \"Bilingual Use -> Card Sort\", \"Bilingual Degree -> Card Sort\",\n",
    "\"Bilingual Status -> Card Sort\", \"                \", \"Bilingual Use -> Flanker\", \"Bilingual Degree -> Flanker\", \"Bilingual Status -> Flanker\",\n",
    "\"                  \", \"Bilingual Use -> Vocabulary\", \"Bilingual Degree -> Vocabulary\", \"Bilingual Status -> Vocabulary\", \"                   \", \"OLS Model\")) +\n",
    " ggtitle(expression(paste(\"Effect Sizes (\", italic(beta), \") and Confidence Intervals Plotted Against the Interval of Equivalence\"))) +\n",
    " theme(panel.border = element_blank(),\n",
    "          panel.grid.major = element_blank(),\n",
    "          axis.line = element_line(size = 1, linetype = \"solid\", colour = \"black\"),\n",
    "          axis.text = element_text(size = 20, colour = \"black\"), plot.title = element_text(hjust = -.5),\n",
    "          )\n",
    " return(p)\n",
    "}\n",
    "\n",
    "tiff(\"figure.tiff\", units = 'in', width = 20, height = 14, res = 200, compression = \"lzw\")\n",
    "credplot.gg(d)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "## Bootstrapping at Small Samples\t  ##\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "\n",
    "samp.n = 30\n",
    "mat_row = 1\n",
    "mat_col = 1\n",
    "nboot = 5000\n",
    "m.colnames<-matrix(nrow=1,ncol = length(ind_dv)*length(ind_iv))\n",
    "mp <- matrix(nrow=nboot, ncol = length(ind_dv)*length(ind_iv)) # set the output matrix size to hold the results\n",
    "\n",
    "for(b in 1:nboot){\n",
    "\tprint(b)\n",
    "\tabcd_subset.samp<-rbind(abcd_subset[sample(which(abcd_subset$bilingual_status=='0'), samp.n), ], abcd_subset[sample(which(abcd_subset$bilingual_status=='1'), samp.n), ])\n",
    "\t\tfor(i in 1:length(ind_dv)){\n",
    "\t\t\tprint(i)\n",
    "\t\t\t\tfor(j in 1:length(ind_iv)){\n",
    "\t\t\t\t\tprint(j)\n",
    "\t\t\t\t\tprediction<-paste(names(abcd_subset)[ind_iv[j]],\"-->\",names(abcd_subset)[ind_dv[i]])\n",
    "\t\t\t\t\tprint(prediction)\n",
    "\t\t\t\t\tform = paste(names(abcd_subset)[ind_dv[i]],\"~\" ,names(abcd_subset)[ind_iv[j]])\n",
    "\t\t\t\t\tform.simple = formula(form)\n",
    "\t\t\t\t\tmod<-lm(form.simple, data = abcd_subset.samp)\n",
    "\t\t\t\t\tprint(summary(mod))\n",
    "\t\t\t\t\tp.value<-summary(mod)$coefficients[8]\n",
    "\t\t\t\t\toutput.p<-p.value\n",
    "\t\t\t\t\tmp[mat_row,mat_col]<-output.p # place the output into the matrix that was dimensioned above\n",
    "\t\t\t\t\tm.colnames[,mat_col]<-prediction\n",
    "\t\t\t\t\tmat_col = mat_col + 1 # step to the next matrix column\n",
    "\t\t\t\t\t}\n",
    "\t\t} # go to the next step in the loop\n",
    "\t\tmat_col = 1\n",
    "\t\tmat_row = mat_row + 1 # step to the next matrix column\n",
    "\tcolnames(mp)<-c(\"Bilingual Status --> Vocabulary\", \"Bilingual Degree --> Vocabulary\", \"Bilingual Use --> Vocabulary\", \"Bilingual Status --> Flanker\", \"Bilingual Degree --> Flanker\", \"Bilingual Use --> Flanker\", \"Bilingual Status --> Card Sort\", \"Bilingual Degree --> Card Sort\", \"Bilingual Use --> Card Sort\", \"Bilingual Status --> SSRT\", \"Bilingual Degree --> SSRT\", \"Bilingual Use --> SSRT\")\n",
    "\t#print(mp)\n",
    "\t#print(mdf)\n",
    "}\n",
    "\n",
    "\n",
    "for.hist <- melt(mp)\n",
    "for.hist<-for.hist[which(for.hist$value < .05),]\n",
    "\n",
    "tiff(\"figure.tiff\", units = 'in', width = 12, height = 10, res = 300, compression = \"lzw\")\n",
    "#par(mar = c(5, 5, 8, 8), xpd=FALSE) #set figure boundaries\n",
    "ggplot(for.hist,aes(x = for.hist$value)) +\n",
    "    facet_wrap(~Var2,scales = \"free_x\", nrow = 4, ncol = 3) + theme_bw() + geom_histogram(\n",
    "    col = \"grey\", bins = 20,\n",
    "    fill=\"light blue\") +\n",
    "    labs(x=expression(paste(italic(\"p \"), \"Value\")), y=\"Frequency\") +\n",
    "    ggtitle(expression(paste(\"Histograms for \", italic(\"p \"), \"values < .05 for \", italic(\"n \"), \"= 30, out of 5000 Bootstrap Replicates\"))) +\n",
    "  \ttheme(plot.title = element_text(hjust = 0.5), text = element_text(size=20), axis.text = element_text(size = 14, colour = \"black\"))\n",
    "dev.off()\n",
    "\n",
    "\n",
    "#################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
